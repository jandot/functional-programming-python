{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i0u19a Data Processing KU Leuven\n",
    "\n",
    "# Python Spark exercises\n",
    "\n",
    "###### _Thomas Moerman, Jan Aerts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we instantiate a SparkContext object and store it in variable 'sc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# prevents ValueError from occurring when reloading this cell\n",
    "def init_sc(prev_sc):  \n",
    "    try:        \n",
    "        return pyspark.SparkContext('local[*]')\n",
    "    except ValueError:\n",
    "        return prev_sc        \n",
    "\n",
    "sc = init_sc(locals().get('sc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are the main building block in Spark programs. They represent distributed computations, partitioned over a cluster of machines. For now, don't worry too much about the technical details. We look at examples of increasing difficulty to build up your intuition on how to write Spark programs.\n",
    "\n",
    "## Getting Sparky\n",
    "\n",
    "We turn a range of 1000 integers in an RDD with the `parallelize(data)` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Spark doesn't compute any results right away. Think of it as storing the \"recipe\" for the computation in a variable.\n",
    "\n",
    "To actually launch a computation, we can perform a `collect` or a `take` operation on the rdd. Let's try taking the top 3 elements from the rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy!\n",
    "\n",
    "Now, why is this approach a convenient? Think about it for few moments.\n",
    "\n",
    "Imagine you wrote a complicated computation on a large dataset. This could take a while to compute. When writing and testing a computation, we can go along while only inspecting a small subset of the result by performing for example a `take(5)` operation. This way we can avoid always having to wait for the entire computation to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "Our boss decided that our sequence should not start at 0 but at 10. Of course we could re-initialize the rdd with the correct range, but let's for the sake of the exercise transform the `rdd` data set.\n",
    "\n",
    "We need a function that allows us to increment each number in the sequence with 10.\n",
    "\n",
    "That function is the `map` function, which takes a function as argument. Let's first define that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inc10(i):\n",
    "    return i + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `inc10` function as argument to the `map` method on our rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_inc10 = rdd.map(inc10)\n",
    "\n",
    "rdd_inc10.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. \n",
    "\n",
    "Now, instead of passing a named function, we can also pass an anonymous function function to the `map` method. Python uses lambda expressions for inline anonymous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_inc10_anon = rdd.map(lambda i: i + 10)\n",
    "\n",
    "rdd_inc10_anon.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that works as well. Often it is more convenient to use anonymous functions, but it all depends on the preference of the programmer of course, there's no right or wrong in this matter.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Let's now make our range of numbers a bit more interesting. We are now only interested in even numbers. In order to express this, we need to `filter` our rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_even(x):\n",
    "    return x % 2 == 0 # complete this\n",
    "\n",
    "rdd_even = rdd_inc10.filter(is_even) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter` method takes a function or lambda expression that takes an integer `i` and produces a `boolean` value that decides whether or not to keep the value `i` in the resulting rdd.\n",
    "\n",
    "Let's test by taking a random sample of 3 items from the rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[596, 310, 60]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_3 = rdd_even.takeSample(False, 3)\n",
    "\n",
    "for i in even_3:\n",
    "    assert is_even(i)\n",
    "    \n",
    "even_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only even numbers, right? Good.\n",
    "\n",
    "### Reducing\n",
    "\n",
    "Let's now perform an aggregation on the data. The workhorse function for simple aggregations is the `reduce(f)` function. Look up the `reduce` function in the [pyspark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) page. It says:\n",
    "\n",
    "**`reduce(f)`**\n",
    "\n",
    "> Reduces the elements of this RDD using the specified _commutative and associative binary operator_.\n",
    "\n",
    "A binary operator is a function that takes 2 inputs and produces one output. Inputs and outputs have the same type. Can you come up with some examples of binary operators?\n",
    "\n",
    "Straightforward examples are mathematical operators like addition (`+`) and multiplication (`*`). Note that these operators are not restricted to numbers, we can also define commutative and associative binary operators on other data types like hashmaps, as we'll see in a later example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete following reduction with the appropriate lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_even = rdd_even.reduce(lambda x, y: x + y) # complete this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check the result (`assert` complains when the answer is wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert sum_even == 254500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do something a bit more difficult. We want to calculate the average of the even numbers, using a single reduction. Instead of working with integers, we will work with the [tuple](http://www.tutorialspoint.com/python/python_tuples.htm) data type, representing pairs of (total, count).\n",
    "\n",
    "Complete the commutative and associative binary operator `sum_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_pairs(pair1, pair2):\n",
    "    # complete this\n",
    "    x, i = pair1\n",
    "    y, j = pair2\n",
    "    return (x + y, i + j)\n",
    "\n",
    "total1, count1 = rdd_even.map(lambda x: (x, 1)).reduce(sum_pairs)\n",
    "\n",
    "avg1 = total1 / count1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert avg1 == 509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise is perhaps the most typical example of a **Map/Reduce** application.\n",
    "\n",
    "We **map** the inputs into a convenient shape, and **reduce** these shapes into the final result.\n",
    "\n",
    "### Aggregating\n",
    "\n",
    "Spark provides several aggregation methods that provide the scaffolding for performing Map/Reduce operations. We now explore on of these methods, called `aggregate`. Let's check out the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) again. \n",
    "\n",
    "**`aggregate(zeroValue, seqOp, combOp)`**\n",
    "\n",
    "The example in the `aggregate` documentation should make sense now. Although the answer is already given, let's do it one more time to train our Spark muscle memory. Show me [wax on, wax off](https://youtu.be/2ynryUjGFt8?t=176)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_value = (0, 0)\n",
    "\n",
    "def seqOp(pair, x):\n",
    "    # complete this\n",
    "    total, count = pair\n",
    "    return (total+x, count+1)\n",
    "\n",
    "total2, count2 = rdd_even.aggregate(zero_value, seqOp, sum_pairs)\n",
    "\n",
    "avg2 = total2 / count2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting average should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert avg2 == 509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien.\n",
    "\n",
    "### Aggregating by key\n",
    "\n",
    "A very common principle in data processing is *aggregation by key*. Before we perform an aggregation by key, we need to introduce another Spark concept, the `PairRDD`. A `PairRDD` is an RDD where each item consists of a pair, or tuple of size 2. In such a tuple, the first element is considered the key, the second element the value.\n",
    "\n",
    "Let's illustrate with a simple example. We use the original rdd and transform it into a new shape, where each integer is keyed by a boolean that says whether it is even or not. We can reuse the `is_even(x)` function we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_by_key = rdd.keyBy(lambda x: is_even(x)) # Complete this\n",
    "\n",
    "assert rdd_by_key.take(3) == [(True, 0), (False, 1), (True, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a reduction by key that returns the sum of even and odd integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_by_key = rdd_by_key.reduceByKey(lambda x, y: x + y).collect() # complete this\n",
    "\n",
    "assert sum_by_key == [(False, 250000), (True, 249500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last aggregation method we will look at for now is the aggregateByKey method. Again, consult the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD).\n",
    "\n",
    "**`aggregateByKey(zeroValue, seqFunc, combFunc)`**\n",
    "\n",
    "Use this to calculate the averages by key instead of the sum by key. We break up the calculation in two steps: first we calculate the sum and count by key, next we calculate the average by key, by using the `mapValues` method. \n",
    "\n",
    "`mapValues` transforms only the \"value\" elements (right) in a `PairRDD`, the keys (left) remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_count_by_key = rdd_by_key.aggregateByKey((0,0), seqOp, sum_pairs) # Complete this\n",
    "\n",
    "assert sum_count_by_key.collect() == [(False, (250000, 500)), (True, (249500, 500))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_count_to_avg(sum_count):\n",
    "    sum, count = sum_count # Complete this\n",
    "    return sum / count\n",
    "\n",
    "avg_by_key = sum_count_by_key.mapValues(sum_count_to_avg).collect() # Complete this\n",
    "\n",
    "assert avg_by_key == [(False, 500.0), (True, 499.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent.\n",
    "\n",
    "We now have some useful methods in our Spark toolbox. We know how to `map`, `filter`, `reduce` and `aggregate`, and we know how to aggregate by key.\n",
    "\n",
    "That made me thirsty, are you thirsty? \n",
    "\n",
    "Let's move on to the beer dataset :-)\n",
    "\n",
    "## Dos cervezas, por favor!\n",
    "\n",
    "We start with reading in the `beers.csv` data set. For convenience, we remove the header from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_header(rdd):    \n",
    "    def drop_first(idx, it):\n",
    "        if idx == 0:\n",
    "            next(it)\n",
    "        return it\n",
    "\n",
    "    return rdd.mapPartitionsWithIndex(drop_first)\n",
    "\n",
    "rdd_beers_header = sc.textFile(\"/usr/local/data/beer/beers.csv\")\n",
    "\n",
    "header = rdd_beers_header.first()\n",
    "\n",
    "rdd_beers_unparsed = drop_header(rdd_beers_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which columns does our data set contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',Merk,Soort,Percentagealcohol,Brouwerij'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Dutch for: \n",
    "\n",
    "`brand, kind, alcohol percentage, brewery`. \n",
    "\n",
    "The first column isn't named but represents record IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1,3 SchtÃ©ng,hoge gisting,6,Brasserie Grain d'Orge\",\n",
       " \"2,400,blond,5.6,'t Hofbrouwerijke voor Brouwerij Montaigu\",\n",
       " '3,IV Saison,saison,6.5,Brasserie de Jandrain-Jandrenouille']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_beers_unparsed.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each line in the RDD is just a String for now. This isn't very handy. Let's parse every line into a tuple of size 5: \n",
    "\n",
    "**`(ID, brand, kind, alcohol percentage, brewery)`.**\n",
    "\n",
    "Notice the `parse_pct` function. This function performs necessary *data cleaning* because the data set contains values in the alcohol percentage column that are not floating point numbers ('NA', 'alcoholvrij', etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '3 SchtÃ©ng', 'hoge gisting', 6.0, \"Brasserie Grain d'Orge\"),\n",
       " (2, '400', 'blond', 5.6, \"'t Hofbrouwerijke voor Brouwerij Montaigu\"),\n",
       " (3, 'IV Saison', 'saison', 6.5, 'Brasserie de Jandrain-Jandrenouille')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_pct(x):\n",
    "    if x == 'NA':            # unknown alcohol percentage\n",
    "        return -1.0\n",
    "    elif 'alcoholvrij' in x: # alcohol free\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(x)    \n",
    "\n",
    "def parse_beer(a):    \n",
    "    try:\n",
    "        return (int(a[0]), a[1], a[2], parse_pct(a[3]), a[4])\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "rdd_beers = rdd_beers_unparsed.map(lambda s: s.split(\",\")).map(parse_beer) # complete this\n",
    "\n",
    "rdd_beers.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with the exercises, we define a few accessor functions for our beer tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ID(beer_tuple):\n",
    "    return beer_tuple[0]\n",
    "\n",
    "def brand(beer_tuple):\n",
    "    return beer_tuple[1]\n",
    "\n",
    "def kind(beer_tuple):\n",
    "    return beer_tuple[2]\n",
    "\n",
    "def alcohol_pct(beer_tuple):\n",
    "    return beer_tuple[3]\n",
    "\n",
    "def brewery(beer_tuple):\n",
    "    return beer_tuple[4]\n",
    "\n",
    "def key(pair):\n",
    "    return pair[0]\n",
    "\n",
    "def value(pair):\n",
    "    return pair[1]\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to demonstrate your [Spark-fu](https://youtu.be/6vMO3XmNXe4?t=7) skills on the beer data set!\n",
    "\n",
    "Good luck.\n",
    "\n",
    "### Exercise 1: number of breweries (easy)\n",
    "\n",
    "How many different breweries do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_breweries = rdd_beers.map(brewery).distinct() # complete this\n",
    "\n",
    "nr_breweries = distinct_breweries.count()\n",
    "\n",
    "assert nr_breweries == 362"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: strongest beer (medium)\n",
    "\n",
    "Which beer is the strongest one? (highest alcohol percentage). Use `reduce` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_pct(beer1, beer2):    \n",
    "    if (alcohol_pct(beer1) > alcohol_pct(beer2)):\n",
    "        return beer1\n",
    "    else:\n",
    "        return beer2\n",
    "\n",
    "beer_max_pct = rdd_beers.reduce(max_pct) # complete this\n",
    "\n",
    "# Verify you solution\n",
    "assert beer_max_pct[3] == 26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is the `max(f)` method, where you pass a selector function `f` that is used for comparing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer_max_pct_2 = rdd_beers.max(alcohol_pct) # complete this\n",
    "\n",
    "# Verify your solution\n",
    "assert beer_max_pct_2[3] == 26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: most common alcohol percentages (medium)\n",
    "\n",
    "How many beers are there for each alcohol percentage? Which 3 alcohol percentages are the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Complete this\n",
    "def beers_per_pct():\n",
    "    \n",
    "    zero = 0\n",
    "    \n",
    "    def seqOp(acc, entry):\n",
    "        return acc + 1\n",
    "    \n",
    "    def comboOp(acc1, acc2):\n",
    "        return acc1 + acc2\n",
    "    \n",
    "    def selector(tuple):\n",
    "        return value(tuple)\n",
    "    \n",
    "    return rdd_beers.keyBy(alcohol_pct).aggregateByKey(zero, seqOp, comboOp).sortBy(selector, ascending = False)\n",
    "\n",
    "# Sanity check\n",
    "assert rdd_beers.count() == beers_per_pct().map(value).sum()\n",
    "\n",
    "# Verify you solution\n",
    "assert beers_per_pct().take(3) == [(8.0, 180), (6.5, 150), (5.0, 131)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: brewery with strongest average beer (advanced)\n",
    "\n",
    "Find the brewery that has the highest average alcohol percentage of all beers it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strongest_average():\n",
    "    \n",
    "    zero = (0,0)\n",
    "    \n",
    "    def seqOp(sum_count, beer): # complete this\n",
    "        sum, count = sum_count\n",
    "        return (sum + alcohol_pct(beer), count + 1)\n",
    "    \n",
    "    def comboOp(t1, t2): # complete this\n",
    "        return (t1[0] + t2[0], t1[1] + t2[1])\n",
    "    \n",
    "    def avg(sum_count): \n",
    "        sum, count = sum_count\n",
    "        return sum / count\n",
    "    \n",
    "    return rdd_beers.keyBy(brewery).aggregateByKey(zero, seqOp, comboOp).mapValues(avg).max(value)\n",
    "\n",
    "assert strongest_average() == ('Staminee De Garre (Brouwerij Van Steenberge)', 11.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: breweries with most kinds of beers (advanced)\n",
    "\n",
    "Return the top 5 breweries with most kinds of beers, also return these kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brouwerij Huyghe',\n",
       "  {'Erkend Belgisch Abdijbier',\n",
       "   'Erkend Belgisch Abdijbier;tripel',\n",
       "   'Speciale Belge',\n",
       "   'abdijbier',\n",
       "   'abdijbier;tripel',\n",
       "   'blond',\n",
       "   'bruin',\n",
       "   'donker;hoge gisting',\n",
       "   'fruitbier',\n",
       "   'glutenvrij pilsener',\n",
       "   'hoge gisting',\n",
       "   'honingbier',\n",
       "   'kerstbier',\n",
       "   'pils',\n",
       "   'pils;biologisch',\n",
       "   'roodbruin',\n",
       "   'sterk blond',\n",
       "   'sterk donker',\n",
       "   'tripel',\n",
       "   'witbier'}),\n",
       " ('Brouwerij Alvinne',\n",
       "  {'Brown ale',\n",
       "   'IPA',\n",
       "   'Vlaams oud bruin',\n",
       "   'amber',\n",
       "   'blond',\n",
       "   'donkerblond',\n",
       "   'donkerbruin',\n",
       "   'donkerrood',\n",
       "   'fruitbier;oak aged',\n",
       "   'hoge gisting',\n",
       "   'hoge gisting;tripel',\n",
       "   'imperial stout',\n",
       "   'lambiek',\n",
       "   'lichtamber',\n",
       "   'oak aged',\n",
       "   'quadrupel;oak aged',\n",
       "   'stout',\n",
       "   'tripel',\n",
       "   'winterbier'}),\n",
       " ('Brouwerij Bavik',\n",
       "  {'aged pale',\n",
       "   'alcoholarm',\n",
       "   'blond',\n",
       "   'blond;hoge gisting',\n",
       "   'bokbier',\n",
       "   'bruin',\n",
       "   'hoge gisting',\n",
       "   'hoge gisting;blond',\n",
       "   'oud bruin',\n",
       "   'pils',\n",
       "   'speciaalbier',\n",
       "   'stout',\n",
       "   'tafelbier',\n",
       "   'tripel',\n",
       "   'winterbier',\n",
       "   'witbier',\n",
       "   'witbier;fruitbier'}),\n",
       " ('Brouwerij Strubbe',\n",
       "  {'',\n",
       "   'IPA',\n",
       "   'alcoholarm',\n",
       "   'blond;hoge gisting',\n",
       "   'bruin',\n",
       "   'fruitbier',\n",
       "   'hoge gisting',\n",
       "   'hoge gisting;tripel',\n",
       "   'pils',\n",
       "   'robijnrood;hoge gisting',\n",
       "   'roodbruin;hoge gisting',\n",
       "   'speciaalbier',\n",
       "   'streekbier;amber',\n",
       "   'streekbier;bruin',\n",
       "   'tafelbier',\n",
       "   'witbier'}),\n",
       " ('Brouwerij Van Steenberge',\n",
       "  {'Tripel',\n",
       "   'abdijbier',\n",
       "   'abdijbier;tripel',\n",
       "   'amber',\n",
       "   'blond',\n",
       "   'donker',\n",
       "   'dubbel',\n",
       "   'fruitbier',\n",
       "   'hoge gisting',\n",
       "   'hoge gisting;donker',\n",
       "   'hoge gisting;streekbier',\n",
       "   'pils',\n",
       "   'roodbruin;hoge gisting',\n",
       "   'streekbier',\n",
       "   'tripel',\n",
       "   'witbier'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diverse_breweries():\n",
    "    \n",
    "    zero = set()\n",
    "    \n",
    "    def seqOp(kinds, beer):\n",
    "        kinds.add(kind(beer))\n",
    "        return kinds\n",
    "    \n",
    "    def comboOp(kinds1, kinds2):\n",
    "        [kinds1.add(k) for k in kinds2]\n",
    "        return kinds1\n",
    "    \n",
    "    def selector(pair):\n",
    "        return len(pair[1])    \n",
    "    \n",
    "    return rdd_beers.keyBy(brewery).aggregateByKey(zero, seqOp, comboOp).sortBy(selector, ascending = False)\n",
    "    \n",
    "# Verify your answer\n",
    "assert diverse_breweries().map(key).take(5) == ['Brouwerij Huyghe', 'Brouwerij Alvinne', 'Brouwerij Bavik', 'Brouwerij Strubbe', 'Brouwerij Van Steenberge']\n",
    "\n",
    "# Show us the result\n",
    "diverse_breweries().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!\n",
    "\n",
    "This completes your [Spark-fu](https://youtu.be/84VtdVK2a0A?t=219) training, oh mighty dragon warrior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
